{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "import math\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bertMaskedLM = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "#bertMaskedLM.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score(sentence, lang):\n",
    "    sentence = \"[CLS] \"+sentence+\" [SEP]\"\n",
    "\n",
    "    if lang == \"da\":\n",
    "        prons = [\"sin\", \"sit\", \"sine\"]\n",
    "    if lang == \"ru\":\n",
    "        prons = \"свой,своя́,своё,свои́,своего́,свое́й,своего́,свои́х,своему́,свое́й,своему́,\\\n",
    "                свои́м,своего́,свою,своего́,свои́х,свои́м,свое́й,свои́м,свои́ми,своём,свое́й,\\\n",
    "                своём,свои́х,свои,своей,своем,своего,своего,свои\".lower().split(\",\")\n",
    "\n",
    "    if lang == \"sv\":\n",
    "        prons = [\"sin\", \"sitt\", \"sina\"]\n",
    "\n",
    "    if lang == \"zh\":\n",
    "        prons = \"自己\"\n",
    "\n",
    "    print(\"Tokenizing....\")\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "    segments_ids = [0] * len(tokenize_input)\n",
    "\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    no_pron = True\n",
    "    for i, token in enumerate(tokenize_input):\n",
    "        if token in prons:\n",
    "            pron_index = i\n",
    "            no_pron = False\n",
    "            break\n",
    "        else:pass\n",
    "\n",
    "    if no_pron==True: return \"no pronouns to replace\"\n",
    "\n",
    "    print(\"masking reflexive pronoun.....\")\n",
    "    #slightly different logics for each language\n",
    "    tokenize_mask_male = tokenize_input.copy()\n",
    "    tokenize_mask_female = tokenize_input.copy()\n",
    "    tokenize_mask_refl = tokenize_input.copy()\n",
    "\n",
    "    if lang == \"da\":\n",
    "        tokenize_mask_male[pron_index] = \"hans\"\n",
    "        tokenize_mask_female[pron_index] = \"hendes\"\n",
    "\n",
    "\n",
    "    if lang == \"ru\":\n",
    "        tokenize_mask_male[pron_index] = \"его\"\n",
    "        tokenize_mask_female[pron_index] = \"ее\"\n",
    "\n",
    "    if lang == \"zh\":\n",
    "        tokenize_mask_male = tokenizer.tokenize(sentence.replace(\"自己\",\"他 UNK\" ))\n",
    "        tokenize_mask_female = tokenizer.tokenize(sentence.replace(\"自己\",\"她 UNK\" ))\n",
    "        tokenize_mask_refl = tokenize_input.copy()\n",
    "\n",
    "        print(tokenize_mask_female,tokenize_mask_refl )\n",
    "\n",
    "        truth_index = tokenize_input.index(\"己\")\n",
    "        male_index = tokenize_mask_male.index(\"他\")\n",
    "        female_index = tokenize_mask_female.index(\"她\")\n",
    "\n",
    "    if lang == \"sv\":\n",
    "        tokenize_mask_male[pron_index] = \"hans\"\n",
    "        tokenize_mask_female[pron_index] = \"hennes\"\n",
    "\n",
    "    if lang == \"zh\":\n",
    "\n",
    "        tensor_input_male = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_mask_male)])\n",
    "\n",
    "        tensor_input_female = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_mask_female)])\n",
    "        tensor_truth = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "\n",
    "    else:\n",
    "        tensor_input_male = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_mask_male)])\n",
    "\n",
    "        tensor_input_female = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_mask_female)])\n",
    "        tensor_truth = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "\n",
    "\n",
    "    print(\"predicting...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions_male = bertMaskedLM(tensor_input_male, segments_tensors)[0]\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions_female = bertMaskedLM(tensor_input_female, segments_tensors)[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions_truth = bertMaskedLM(tensor_truth, segments_tensors)[0]\n",
    "\n",
    "    #predicted_male = predictions_male[0, pron_index].unsqueeze(0)\n",
    "    #predicted_female = predictions_female[0, pron_index].unsqueeze(0)\n",
    "    #truth_ = torch.tensor([tensor_truth[0,pron_index].item()])\n",
    "\n",
    "    #loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    #loss_male = loss_fct(predictions_male.squeeze(),tensor_truth.squeeze()).data\n",
    "    loss_male = F.cross_entropy(predictions_male.squeeze(), tensor_truth.squeeze())\n",
    "    #loss_female = loss_fct(predictions_female.squeeze(),tensor_truth.squeeze()).data\n",
    "    loss_female = F.cross_entropy(predictions_female.squeeze(), tensor_truth.squeeze())\n",
    "    #loss_ref = loss_fct(predictions_truth.squeeze(),tensor_truth.squeeze()).data\n",
    "    loss_ref = F.cross_entropy(predictions_truth.squeeze(), tensor_truth.squeeze())\n",
    "    \n",
    "    #print(loss)\n",
    "    return \"male: \"+ str(loss_male.item())+\" \"+ str(torch.exp(loss_male))+ \" female: \"+ str(loss_female.item())+ \" \" +\\\n",
    "            str(torch.exp(loss_female)) + \" refl: \"+ str(loss_ref.item())+ \" \" + str(torch.exp(loss_ref))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing....\n",
      "masking reflexive pronoun.....\n",
      "predicting...\n",
      "Tokenizing....\n",
      "masking reflexive pronoun.....\n",
      "predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing....\n",
      "masking reflexive pronoun.....\n",
      "predicting...\n",
      "Tokenizing....\n",
      "masking reflexive pronoun.....\n",
      "predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing....\n",
      "masking reflexive pronoun.....\n",
      "predicting...\n",
      "Tokenizing....\n",
      "masking reflexive pronoun.....\n",
      "predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:01,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing....\n",
      "masking reflexive pronoun.....\n",
      "predicting...\n",
      "Tokenizing....\n",
      "masking reflexive pronoun.....\n",
      "predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:01,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing....\n",
      "masking reflexive pronoun.....\n",
      "predicting...\n",
      "Tokenizing....\n",
      "masking reflexive pronoun.....\n",
      "predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:01,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing....\n",
      "masking reflexive pronoun.....\n",
      "predicting...\n",
      "Tokenizing....\n",
      "masking reflexive pronoun.....\n",
      "predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:02,  5.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m/work/cool-programmer-astrid/ABC-dataset/outputs/lm/out_\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mlang\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_replicate_new.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     19\u001b[0m     \u001b[39mfor\u001b[39;00m i, sent \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(reflexive_sents)):\n\u001b[0;32m---> 20\u001b[0m         scores \u001b[39m=\u001b[39m score(sent, lang)\n\u001b[1;32m     21\u001b[0m         f\u001b[39m.\u001b[39mwrite(sent \u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m scores \u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[38], line 95\u001b[0m, in \u001b[0;36mscore\u001b[0;34m(sentence, lang)\u001b[0m\n\u001b[1;32m     87\u001b[0m     predictions_truth \u001b[39m=\u001b[39m bertMaskedLM(tensor_truth, segments_tensors)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     89\u001b[0m \u001b[39m#predicted_male = predictions_male[0, pron_index].unsqueeze(0)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m#predicted_female = predictions_female[0, pron_index].unsqueeze(0)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39m#truth_ = torch.tensor([tensor_truth[0,pron_index].item()])\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m \u001b[39m#loss_fct = torch.nn.CrossEntropyLoss()\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m#loss_male = loss_fct(predictions_male.squeeze(),tensor_truth.squeeze()).data\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m loss_male \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mcross_entropy(predictions_male\u001b[39m.\u001b[39;49msqueeze(), tensor_truth\u001b[39m.\u001b[39;49msqueeze())\n\u001b[1;32m     96\u001b[0m \u001b[39m#loss_female = loss_fct(predictions_female.squeeze(),tensor_truth.squeeze()).data\u001b[39;00m\n\u001b[1;32m     97\u001b[0m loss_female \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(predictions_female\u001b[39m.\u001b[39msqueeze(), tensor_truth\u001b[39m.\u001b[39msqueeze())\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3026\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filename = \"/work/cool-programmer-astrid/ABC-dataset/data/COREF_LM/coref_lm.da\"\n",
    "lang = \"da\"\n",
    "\n",
    "reflexive_sents = []\n",
    "with open(filename, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    restart = 0\n",
    "    for line in lines:\n",
    "        if \"--------------\" in line: pass\n",
    "        elif \"---\" in line:\n",
    "            restart = 0\n",
    "        else:\n",
    "            if restart == 0:\n",
    "                reflexive_sents.append(line.strip())\n",
    "                restart = 1\n",
    "\n",
    "with open(\"/work/cool-programmer-astrid/ABC-dataset/outputs/lm/out_\"+lang+\"_replicate_new.txt\", \"w\") as f:\n",
    "    for i, sent in tqdm(enumerate(reflexive_sents)):\n",
    "        scores = score(sent, lang)\n",
    "        f.write(sent +\" \"+ scores +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "590.5804694768062"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.tensor(6.381105899810791)) #590.5804\n",
    "\n",
    "math.exp(6.381105899810791)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "exp(): argument 'input' (position 1) must be Tensor, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#2.408111333847046 11.112952654759475\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m torch\u001b[39m.\u001b[39;49mexp(\u001b[39m2.408111333847046\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: exp(): argument 'input' (position 1) must be Tensor, not float"
     ]
    }
   ],
   "source": [
    "#2.408111333847046 11.112952654759475\n",
    "math.exp(2.408111333847046)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
